{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'response = model.generate_content(\"Tell me a joke about AI.\")\\nprint(response.text)'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=\"---\")\n",
    "\n",
    "model = genai.GenerativeModel(\"gemini-2.0-flash-001\")\n",
    "\n",
    "\"\"\"response = model.generate_content(\"Tell me a joke about AI.\")\n",
    "print(response.text)\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio, Image, Markdown, Video, display\n",
    "from gitingest import ingest\n",
    "from google.genai.types import CreateCachedContentConfig, GenerateContentConfig, Part\n",
    "import nest_asyncio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FailedPrecondition",
     "evalue": "400 The File wy7r6bc4rq0k is not in an ACTIVE state and usage is not allowed.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFailedPrecondition\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 13\u001b[0m\n\u001b[0;32m      4\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mWhat is shown in this video?\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124mWhere should I go to see it?\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124mWhat are the top 5 places in the world that look like this?\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124mProvide the 10 best tags for this video?\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     11\u001b[0m contents \u001b[38;5;241m=\u001b[39m [prompt, uploaded_video]\n\u001b[1;32m---> 13\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m display(Markdown(response\u001b[38;5;241m.\u001b[39mtext))\n",
      "File \u001b[1;32mc:\\projects\\case_study_1\\video\\lib\\site-packages\\google\\generativeai\\generative_models.py:331\u001b[0m, in \u001b[0;36mGenerativeModel.generate_content\u001b[1;34m(self, contents, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001b[0m\n\u001b[0;32m    329\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_types\u001b[38;5;241m.\u001b[39mGenerateContentResponse\u001b[38;5;241m.\u001b[39mfrom_iterator(iterator)\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 331\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mgenerate_content(\n\u001b[0;32m    332\u001b[0m             request,\n\u001b[0;32m    333\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrequest_options,\n\u001b[0;32m    334\u001b[0m         )\n\u001b[0;32m    335\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_types\u001b[38;5;241m.\u001b[39mGenerateContentResponse\u001b[38;5;241m.\u001b[39mfrom_response(response)\n\u001b[0;32m    336\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m google\u001b[38;5;241m.\u001b[39mapi_core\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mInvalidArgument \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\projects\\case_study_1\\video\\lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\client.py:835\u001b[0m, in \u001b[0;36mGenerativeServiceClient.generate_content\u001b[1;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_universe_domain()\n\u001b[0;32m    834\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[1;32m--> 835\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    836\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    837\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    838\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    839\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    840\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    842\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[0;32m    843\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\projects\\case_study_1\\video\\lib\\site-packages\\google\\api_core\\gapic_v1\\method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[1;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[1;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\projects\\case_study_1\\video\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:294\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    290\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    291\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[0;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[0;32m    293\u001b[0m )\n\u001b[1;32m--> 294\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    300\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\projects\\case_study_1\\video\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:156\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[1;32m--> 156\u001b[0m     next_sleep \u001b[38;5;241m=\u001b[39m \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43msleep_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(next_sleep)\n",
      "File \u001b[1;32mc:\\projects\\case_study_1\\video\\lib\\site-packages\\google\\api_core\\retry\\retry_base.py:214\u001b[0m, in \u001b[0;36m_retry_error_helper\u001b[1;34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m predicate_fn(exc):\n\u001b[0;32m    209\u001b[0m     final_exc, source_exc \u001b[38;5;241m=\u001b[39m exc_factory_fn(\n\u001b[0;32m    210\u001b[0m         error_list,\n\u001b[0;32m    211\u001b[0m         RetryFailureReason\u001b[38;5;241m.\u001b[39mNON_RETRYABLE_ERROR,\n\u001b[0;32m    212\u001b[0m         original_timeout,\n\u001b[0;32m    213\u001b[0m     )\n\u001b[1;32m--> 214\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msource_exc\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    216\u001b[0m     on_error_fn(exc)\n",
      "File \u001b[1;32mc:\\projects\\case_study_1\\video\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:147\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 147\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    148\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n\u001b[0;32m    149\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(_ASYNC_RETRY_WARNING)\n",
      "File \u001b[1;32mc:\\projects\\case_study_1\\video\\lib\\site-packages\\google\\api_core\\timeout.py:130\u001b[0m, in \u001b[0;36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    126\u001b[0m         remaining_timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout\n\u001b[0;32m    128\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m remaining_timeout\n\u001b[1;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\projects\\case_study_1\\video\\lib\\site-packages\\google\\api_core\\grpc_helpers.py:78\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m callable_(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m---> 78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[1;31mFailedPrecondition\u001b[0m: 400 The File wy7r6bc4rq0k is not in an ACTIVE state and usage is not allowed."
     ]
    }
   ],
   "source": [
    "\n",
    "local_filename =\"C://projects//case_study_1//archive//z_6gVvQb2d0.mp4\" \n",
    "#display(Video(local_filename, width=350))\n",
    "uploaded_video = genai.upload_file(path=local_filename, mime_type=\"video/mp4\")\n",
    "prompt = \"\"\"What is shown in this video?\n",
    "Where should I go to see it?\n",
    "What are the top 5 places in the world that look like this?\n",
    "Provide the 10 best tags for this video?\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "contents = [prompt, uploaded_video]\n",
    "\n",
    "response = model.generate_content(contents=contents)\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for file to be ACTIVE. Current state: PROCESSING\n",
      "Waiting for file to be ACTIVE. Current state: PROCESSING\n",
      "Waiting for file to be ACTIVE. Current state: PROCESSING\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Here is a summary of the video:\n",
       "The video shows an orange tabby cat, named Miffy, being offered dog food. The speaker in the video seems unsure if Miffy will eat dog food and, after offering it to her, she eats it. To help Miffy with the dog food, she is given a plate with the food on it. Miffy eats most of the dog food and the speaker notes that Miffy does not get sick. The speaker says, “Yes, cats can eat dog food.”"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "import google.generativeai as genai\n",
    "from IPython.display import Markdown\n",
    "import os\n",
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "model = genai.GenerativeModel(\"gemini-2.0-flash-001\")\n",
    "\n",
    "\n",
    "video_path = r\"C:\\projects\\case_study_1\\archive\\-esJrBWj2d8.mp4\"\n",
    "uploaded_video = genai.upload_file(path=video_path, mime_type=\"video/mp4\")\n",
    "\n",
    "\n",
    "while uploaded_video.state.name != \"ACTIVE\":\n",
    "    print(f\"Waiting for file to be ACTIVE. Current state: {uploaded_video.state.name}\")\n",
    "    time.sleep(2)\n",
    "    uploaded_video = genai.get_file(uploaded_video.name)\n",
    "\n",
    "\n",
    "prompt = \"\"\"\n",
    "What is shown in this video? analyze it and provide a summary of the video.\n",
    "\"\"\"\n",
    "\n",
    "response = model.generate_content([prompt, uploaded_video])\n",
    "display(Markdown(response.text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for file to be ACTIVE. Current state: PROCESSING\n",
      "Waiting for file to be ACTIVE. Current state: PROCESSING\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "A family fed their cat, Miffy, dog food to see if it would eat it. The kids thought Miffy would eat the dog food, so the family put it out, and Miffy ate most of the food without getting sick."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "import google.generativeai as genai\n",
    "from IPython.display import Markdown\n",
    "import os\n",
    "from IPython.display import Video\n",
    "def video_summary_local(video_path):\n",
    "    genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "    model = genai.GenerativeModel(\"gemini-2.0-flash-001\")\n",
    "\n",
    "    video_path = r\"C:\\projects\\case_study_1\\archive\\-esJrBWj2d8.mp4\"\n",
    "    uploaded_video = genai.upload_file(path=video_path, mime_type=\"video/mp4\")\n",
    "\n",
    "    while uploaded_video.state.name != \"ACTIVE\":\n",
    "        print(f\"Waiting for file to be ACTIVE. Current state: {uploaded_video.state.name}\")\n",
    "        time.sleep(2)\n",
    "        uploaded_video = genai.get_file(uploaded_video.name)\n",
    "\n",
    "    prompt = \"\"\"\n",
    "    give me a summary of the given video, don't include any other text.\n",
    "    \"\"\"\n",
    "\n",
    "    response = model.generate_content([prompt, uploaded_video])\n",
    "    display(Markdown(response.text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument: 'https:\\\\storage.googleapis.com\\\\github-repo\\\\img\\\\gemini\\\\multimodality_usecases_overview\\\\mediterraneansea.mp4'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      5\u001b[0m video_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/mediterraneansea.mp4\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 6\u001b[0m uploaded_video \u001b[38;5;241m=\u001b[39m \u001b[43mgenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupload_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvideo_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmime_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvideo/mp4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m genai\u001b[38;5;241m.\u001b[39mconfigure(api_key\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGOOGLE_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m      9\u001b[0m model \u001b[38;5;241m=\u001b[39m genai\u001b[38;5;241m.\u001b[39mGenerativeModel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemini-2.0-flash-001\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\projects\\case_study_1\\video\\lib\\site-packages\\google\\generativeai\\files.py:85\u001b[0m, in \u001b[0;36mupload_file\u001b[1;34m(path, mime_type, name, display_name, resumable)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m name:\n\u001b[0;32m     83\u001b[0m     name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiles/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 85\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmime_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmime_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisplay_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisplay_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresumable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresumable\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m file_types\u001b[38;5;241m.\u001b[39mFile(response)\n",
      "File \u001b[1;32mc:\\projects\\case_study_1\\video\\lib\\site-packages\\google\\generativeai\\client.py:116\u001b[0m, in \u001b[0;36mFileServiceClient.create_file\u001b[1;34m(self, path, mime_type, name, display_name, resumable, metadata)\u001b[0m\n\u001b[0;32m    112\u001b[0m     media \u001b[38;5;241m=\u001b[39m googleapiclient\u001b[38;5;241m.\u001b[39mhttp\u001b[38;5;241m.\u001b[39mMediaIoBaseUpload(\n\u001b[0;32m    113\u001b[0m         fd\u001b[38;5;241m=\u001b[39mpath, mimetype\u001b[38;5;241m=\u001b[39mmime_type, resumable\u001b[38;5;241m=\u001b[39mresumable\n\u001b[0;32m    114\u001b[0m     )\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 116\u001b[0m     media \u001b[38;5;241m=\u001b[39m \u001b[43mgoogleapiclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhttp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMediaFileUpload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmimetype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmime_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresumable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresumable\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_local\u001b[38;5;241m.\u001b[39mdiscovery_api\u001b[38;5;241m.\u001b[39mmedia()\u001b[38;5;241m.\u001b[39mupload(body\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m: file}, media_body\u001b[38;5;241m=\u001b[39mmedia)\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m metadata:\n",
      "File \u001b[1;32mc:\\projects\\case_study_1\\video\\lib\\site-packages\\googleapiclient\\_helpers.py:130\u001b[0m, in \u001b[0;36mpositional.<locals>.positional_decorator.<locals>.positional_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m positional_parameters_enforcement \u001b[38;5;241m==\u001b[39m POSITIONAL_WARNING:\n\u001b[0;32m    129\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[1;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\projects\\case_study_1\\video\\lib\\site-packages\\googleapiclient\\http.py:594\u001b[0m, in \u001b[0;36mMediaFileUpload.__init__\u001b[1;34m(self, filename, mimetype, chunksize, resumable)\u001b[0m\n\u001b[0;32m    592\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    593\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filename \u001b[38;5;241m=\u001b[39m filename\n\u001b[1;32m--> 594\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    595\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mimetype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;66;03m# No mimetype provided, make a guess.\u001b[39;00m\n\u001b[0;32m    597\u001b[0m     mimetype, _ \u001b[38;5;241m=\u001b[39m mimetypes\u001b[38;5;241m.\u001b[39mguess_type(filename)\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 22] Invalid argument: 'https:\\\\storage.googleapis.com\\\\github-repo\\\\img\\\\gemini\\\\multimodality_usecases_overview\\\\mediterraneansea.mp4'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import google.generativeai as genai\n",
    "from IPython.display import Markdown\n",
    "import os\n",
    "video_url = \"https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/mediterraneansea.mp4\"\n",
    "uploaded_video = genai.upload_file(path=video_url, mime_type=\"video/mp4\")\n",
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "model = genai.GenerativeModel(\"gemini-2.0-flash-001\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"while uploaded_video.state.name != \"ACTIVE\":\n",
    "    print(f\"Waiting for file to be ACTIVE. Current state: {uploaded_video.state.name}\")\n",
    "    time.sleep(2)\n",
    "    uploaded_video = genai.get_file(uploaded_video.name)\"\"\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "give me a summary of the given video, don't include any other text.\n",
    "\"\"\"\n",
    "\n",
    "response = model.generate_content([prompt, uploaded_video])\n",
    "display(Markdown(response.text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello everyone I am Santi and I'm currently working as an ml engineer today we are going to do an awesome machine learning project that you can add to your resume and impress all the interviewers um I'm dedicated to teaching machine learning to all of you and to ensure that you all learned an ml job as soon as possible okay so without any further Ado let's Dive Right In so our title is going to be um so our project title is going to be uh YouTube video summarizer with llm well so what's going to do is going to take in a video and you can ask any question regarding the videos contents the images Etc and that's going to be really really good really useful as well and that can be a mini deployed project as well okay so first things first um let's let's let's think about why do we need this in the first place the interviewer might say why not just use chat GPT right because chat gpg has now enabled the search web feature for every free user right so you might be wondering why not just use that okay so let's see why we are not going to use that as you can see right here we have our chat GPT open and let's see um which video are we going to use yeah so this is a video that we are going to use right here as you can see my binary classification video which is of the length of 32 32 minutes okay so let's put this inside the chat GPD and I will the search web feature and say write the transcript for this this for this video because first of all we'll need the transcript right as you can see right here it says that it's unable to actually get the video um transcript so if you don't have the transcript obviously then you won't be able to do anything with it right okay whenever you get any project it's very important to understand that you first need to break the project down into smaller pieces that you can tackle easily this is very important because if you think that how how am I going to do that we don't have this we don't have that at the first so this going to be very confusing so let's tackle it one by one step by step remember this for any project that you are going to do okay I'm going to use Google collab here because we need GPU for our support so if you don't have GPU on your system or you're using any Windows system you're using a Mac system whatever it is it doesn't matter as long as you have Google collab you can just go in it's free for you to use and you can do the project here and in your resume you can just put in link okay so let's do first install let's first install the YouTube transcript API great now we have the YouTube trans transcript API so now what we going to do is basically get the link provide the link to the video um to this library and generate the transcript using this now we're going to import the YouTube transcript API so from YouTube YouTube transcript API sounds good now let's get the video ID from the um video so what we going to do is basically as you know this is the video right here and this is the video ID so whenever you put in a link you're going to get the video ID from here get video ID is going to be this function great you now have the video ID so let's see what the video ID looks like um copy this yeah this is the video ID yeah next thing is getting the transcript so basically get video ID we already have that so let's just store it in a variable this one going to be video ID um yeah sounds good now let's see what the transcript looks like yeah as you can see this is what the transcript looks like so now what you're going to do is uh add all this together yep yeah so now what we going to do is do the transcript join so what does it going to look like exactly just join this together all the text features that you have in this transcript great now we can see what the transcript joint looks like yeah great as you can see right here this is our transcript that we have um okay as you can see we have it Santi but we can't do anything about it so let's just forget about it for the moment yeah um sounds good now what we going to do is basically we need to as you can see the transcript that you saw um it does not actually have a proper punctuation so we going to put in the punctuation now how we're going to do that we're going to use a model for that this is where the llm comes into the play okay so now as you can see like uh restoring the punctuation this is called a library which we have actually and this is actually a restore Punk Library yeah this is our Punk library that we have actually but there's a problem out here the versions are very like old because it's three years ago so that is why there's a patch for the Same by another uh GitHub user so that that is what we are going to use here great so this is actually the thing please note it down because you're going to need this if you use the normal ARB it's not going to help great now the next thing comes is restoring the punctuations so from the r pun we going to import restore puns and then you're going to use this restore Punk function so what this actually does behind the scenes is it uses a model from the hugging face library and this model is actually loaded here and this model is actually pre-trained for taking in bunch of text and then um getting the punctu ation back to that text great now that this is done let's move on to the um getting the results from the punctuation so as you can see right here this one py toch model this model actually got loaded so one thing if you facing some problem with GPU it's very important to use this actually um here yeah as you can see right here we have GPU enable T4 so you need to enable the GPU in your um collab otherwise you're going to get an error that is GPU is not available and it's not really possible to do this without GPU you can do it with CPU as well but it's going to be very slow okay so now that you can see we have loaded the model we have gotten all this vocabulary we have gotten the tokenizer we have gotten the configuration and everything so now we are ready to go let's see the results yeah we are printing out the results as you can see yeah this is as you can see it's like very good punctuation I'm an algorithm full stop I've covered log likelihood it which is kind of a prerequisite full stop then comma a du let start it's pretty good not um 100% accurate because it's not a very good model but it's pretty good we are going to use this okay so the next thing that we're going to do is um we're going to use the chat GPT free version API version to get the results from this transcript so as you can see we have the transcript right here so we know the entire content of the video so it's going to be piece of cake for now sounds good yeah so let's see we going to import open AI as you can see I've already install open a if you have not then you can do it using not equal to pip install open AI to the open AI API key website yeah as you can see right here this has API key you can just log in yeah you can just log in here's my API key you can't see that just create a new secret key write the key permissions all create secret key and you have the API key right here it's a it's it's very easy sounds good now now we have the API key now we are having the prompt so let's have the prompt here so as you can see right here let's go over this so from open we importing open then we have the client with the API key just now we set and now this is the remember this this is very very easy and this is like very standard thing that we going to do so this is the way you have to access the uh openai API so we have the message here roll user content prompt so this is the prompt that we going to pass in um and then we are going to use a model GPT 3.5 turbo because the other GPD models are all paid we're not going to use a paid version we're going to use the free version and we have the temperature set equal to one it controls a Randomness basically Max tokens 2 56 is the maximum number of tokens in the response um top P equal to 1 which means the op is a nuclear sampling parameter which is just another thing for uh like temperature just like temperature we we are determining the one with the higher probability Mass um frequency penalty is basically zero which means that we don't want the model to repeat anything and prence penalty is also zero which means that we do not want it to add unnecessary words or just making it for veros without any reason for example if you're asking for one word just reply with one word don't be any like verbos and explain why you're saying that and all that you don't want that right um so let's run this great now we have the response right here simple thing just print it out so this is the way you can print out the response is very important chat completion. choices z. message. content this is the way it actually comes in so if you want to see what the chat completion looks like you can check it out it's actually kind of a dictionary uh it's kind of a class here where you have different attributes so this is the content that we want um so that is why we are doing this so let's see the content so the test discusses the derivation and maths focusing on binary classification perfect destion boundary uh like likelihood function sigmoid function gradient descent optimal parameters like Theta it controls by discussing how to determine the equation in theeta transpose X comprehensive explanation and its implementation so well it's your cue to go and study this classification um video as well but yeah you can do it at your own pace but it's a very important thing as well sorry not sorry a little bit of promotion but yeah now we done so now you can just take on a prompt which is any kind of prompt that you want uh why don't why just change this let's just have this one right here so let's change it to I don't know anything you want maybe um answer questions based on this text you have right here so let's add a question um what do you want me to add let's say what is classification what is exactly taught here let's see what it has to say algorithm predict the class speaker well it's not my name but whatever we have put it in the transcript so derivation and math specifically binary classification uh okay my gender is also wrong but whatever decision boundary and then dering the this one radiant descent see is perfect the answer is perfect key Concepts it um and steps involving binary classification retailed description as you can see this is extremely good um I also show you how you can deploy this from n to end but that's for another video okay take care bye\n",
      "Santi, an ML engineer, introduces a project: a YouTube video summarizer using an LLM. The goal is to allow users to ask questions about video content. He explains why using this project is better than simply using ChatGPT's search web feature (because ChatGPT can't directly access video transcripts).\n",
      "\n",
      "He emphasizes breaking down the project into smaller, manageable steps. He uses Google Colab (due to GPU needs) and starts by installing the YouTube Transcript API. He then demonstrates how to extract the video ID and generate a transcript.\n",
      "\n",
      "The transcript initially lacks punctuation, so he uses the \"restore-punk\" library (using a patched version due to outdated versions) to add punctuation. This library utilizes a pre-trained Hugging Face model.\n",
      "\n",
      "Finally, he uses the OpenAI API (GPT-3.5 Turbo, the free version) to answer questions based on the punctuated transcript. He explains the key parameters in the API request (temperature, max_tokens, etc.) and shows how to access the response content. He then demonstrates how to ask questions about the video content and get accurate answers based on the transcript. He mentions deployment will be covered in another video.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "video_url = \"https://www.youtube.com/watch?v=xDQL3vWwcp0\"\n",
    "def get_video_id(url_link):\n",
    "  return url_link.split(\"watch?v=\")[-1]\n",
    "\n",
    "def video_summary_yt(video_url):\n",
    "  genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "  model = genai.GenerativeModel(\"gemini-2.0-flash-001\")\n",
    "  video_id = get_video_id(video_url)  # From your YouTube URL\n",
    "  transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "\n",
    "  # Combine transcript text\n",
    "  text = \" \".join([entry['text'] for entry in transcript])\n",
    "  print(text)\n",
    "\n",
    "\n",
    "  response = model.generate_content(f\"Summarize this video transcript:\\n\\n{text}\")\n",
    "  return response.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
